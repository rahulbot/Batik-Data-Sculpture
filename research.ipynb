{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batik Resilience Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mediacloud, datetime, time, json, re, random, os, time, csv, mediacloud.api, operator\n",
    "from datetime import date, timedelta\n",
    "mc = mediacloud.api.AdminMediaCloud(os.environ['MC_API_KEY'])\n",
    "mediacloud.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = \"publish_day:[2017-01-01T00:00:00Z TO 2019-12-01T00:00:00Z]\"\n",
    "us_media = ['34412234', '38379429']\n",
    "places = {\n",
    "    'Sydney': ['34412282', '38378024'],\n",
    "    'Chennai': ['34412118', '38379954'],\n",
    "    'Florida': ['38379430'],\n",
    "    'Alaska': ['38381315'],\n",
    "    'Kenya': ['34412126', '38380260'],\n",
    "    'Singapore': ['34412474'],\n",
    "    'London': ['34412476', '38381111'],\n",
    "    'Brazil': ['34412257', '38379250'],\n",
    "    'Islands': [ '34412175', '34412204', '34412411', '38381481', '34412109', '34412204', '34412168', '34412399', '38380877', '34412137' ],\n",
    "    '\"Saudi Arabia\"': ['34412050', '38380804'],\n",
    "    # 'Argentina': ['34412043', '38376412'],\n",
    "    # 'Paris': ['34412146', '38379799'],\n",
    "    # 'Amsterdam': ['34412382', '38380454'],\n",
    "    # 'Rome': ['34412372', '38380117'],\n",
    "    # 'Milan': ['34412372', '38380117'],\n",
    "    # 'Madrid': ['34412356', '38002034'],\n",
    "    # 'Athens': ['34412477', '38379845'],\n",
    "    #'Mexico': ['34412427', '38380322'],\n",
    "    # 'Rotterdam': ['34412382', '38380454'],\n",
    "    \n",
    "}\n",
    "queries = {\n",
    "    'security': '\"security climate\"~40',\n",
    "    'resilience': '\"resilience climate\"~40',\n",
    "    'migration': '((\"migration climate\"~40) OR  (\"migrant climate\"~40) OR  (\"migrate climate\"~40)  OR  (\"immigration climate\"~40)  OR  (\"immigrate climate\"~40))'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Batik Length Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = []\n",
    "for p, m in places.items():\n",
    "    for t, q in queries.items():\n",
    "        local_relevant_story_count = mc.storyCount(\"{} AND {} AND language:en AND tags_id_media:({})\".format(p,q,\" \".join(m)), solr_filter=timespan)\n",
    "        local_total_story_count = mc.storyCount(\"{} AND language:en AND tags_id_media:({})\".format(p,\" \".join(m)), solr_filter=timespan)\n",
    "        us_relevant_story_count = mc.storyCount(\"{} AND {} AND language:en AND tags_id_media:({})\".format(p,q,\" \".join(us_media)), solr_filter=timespan)\n",
    "        us_total_story_count = mc.storyCount(\"{} AND language:en AND tags_id_media:({})\".format(p,\" \".join(us_media)), solr_filter=timespan)\n",
    "        row = {\n",
    "            'place': p,\n",
    "            'topic': t,\n",
    "            'local_relevant': local_relevant_story_count['count'],\n",
    "            'local_total': local_total_story_count['count'],\n",
    "            'us_relevant': us_relevant_story_count['count'],\n",
    "            'us_total': us_total_story_count['count'],\n",
    "            'combined_relevant': local_relevant_story_count['count'] + us_relevant_story_count['count'],\n",
    "            'combined_total': local_total_story_count['count'] + us_total_story_count['count'],\n",
    "        }\n",
    "        length_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-length.json', 'w') as f:\n",
    "    f.write(json.dumps(length_data))\n",
    "with open('batik-data/batik-length.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter = csv.DictWriter(csvfile, fieldnames=['place', 'topic', 'local_relevant', 'local_total', \n",
    "                                                    'us_relevant', 'us_total', 'combined_relevant', 'combined_total'])\n",
    "    spamwriter.writeheader()\n",
    "    for row in length_data:\n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Batik Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_LABELS_TAG_SET_ID = 1963  # the tag set all the descriptor tags are in\n",
    "theme_data = []\n",
    "for p, m in places.items():\n",
    "    for t, q in queries.items():\n",
    "        top_themes = mc.storyTagCount(\"{} AND {} AND language:en AND tags_id_media:({})\".format(p,q,\" \".join(m)), solr_filter=timespan, tag_sets_id=NYT_LABELS_TAG_SET_ID)\n",
    "        item = {\n",
    "            'place': p,\n",
    "            'topic': t,\n",
    "            'top_themes': [{'count': tag['count'], 'name': tag['description'], 'tags_id': tag['tags_id']} for tag in top_themes]\n",
    "        }\n",
    "        theme_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "with open('batik-data/batik-themes.json', 'w') as f:\n",
    "    f.write(json.dumps(theme_data))\n",
    "with open('batik-data/batik-themes.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow([\"{} - {}\".format(i['place'], i['topic']) for i in theme_data])\n",
    "    for idx in range(0, 20):\n",
    "        spamwriter.writerow([i['top_themes'][idx]['name'] for i in theme_data if idx < len(i['top_themes'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Climate Coverage Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_data = []\n",
    "for p, m in places.items():\n",
    "    climate_story_count = mc.storyCount('{} AND (\"climate change\" OR \"global warming\") AND language:en AND tags_id_media:({})'.format(p,\" \".join(m)), solr_filter=timespan)\n",
    "    security_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['security'],\" \".join(m)), solr_filter=timespan)\n",
    "    resilience_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['resilience'],\" \".join(m)), solr_filter=timespan)\n",
    "    migration_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['migration'],\" \".join(m)), solr_filter=timespan)\n",
    "    total_story_count = mc.storyCount(\"{} AND language:en AND tags_id_media:({})\".format(p,\" \".join(m)), solr_filter=timespan)\n",
    "    row = {\n",
    "        'place': p,\n",
    "        'security-stories': security_story_count['count'],\n",
    "        'resilience-stories': resilience_story_count['count'],\n",
    "        'migration-stories': migration_story_count['count'],\n",
    "        'climate-change-stories': climate_story_count['count'],\n",
    "        'total-stories': total_story_count['count'],\n",
    "    }\n",
    "    climate_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-climate-coverage.json', 'w') as f:\n",
    "    f.write(json.dumps(climate_data))\n",
    "with open('batik-data/batik-climate-coverage.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.DictWriter(csvfile, fieldnames=[\"place\", \"security-stories\", \"resilience-stories\", \"migration-stories\", \"climate-change-stories\", \"total-stories\"])\n",
    "    spamwriter.writeheader()\n",
    "    for row in climate_data:\n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Climate Coverage Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_attention_data = []\n",
    "for p, m in places.items():\n",
    "    climate_story_count = mc.storyCount('{} AND (\"climate change\" OR \"global warming\") AND language:en AND tags_id_media:({})'.format(p,\" \".join(m)), solr_filter=timespan, split=True, split_period='week')\n",
    "    security_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['security'],\" \".join(m)), solr_filter=timespan, split=True, split_period='week')\n",
    "    resilience_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['resilience'],\" \".join(m)), solr_filter=timespan, split=True, split_period='week')\n",
    "    migration_story_count = mc.storyCount('{} AND {} AND language:en AND tags_id_media:({})'.format(p,queries['migration'],\" \".join(m)), solr_filter=timespan, split=True, split_period='week')\n",
    "    total_story_count = mc.storyCount(\"{} AND language:en AND tags_id_media:({})\".format(p,\" \".join(m)), solr_filter=timespan, split=True, split_period='week')\n",
    "    row = {\n",
    "        'place': p,\n",
    "        'security-stories': security_story_count,\n",
    "        'resilience-stories': resilience_story_count,\n",
    "        'migration-stories': migration_story_count,\n",
    "        'climate-change-stories': climate_story_count,\n",
    "        'total-stories': total_story_count,\n",
    "    }\n",
    "    climate_attention_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-climate-attention.json', 'w') as f:\n",
    "    f.write(json.dumps(climate_attention_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words for each Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = []\n",
    "word_query = \"{} AND {} AND language:en AND tags_id_media:({})\"\n",
    "for p, m in places.items():\n",
    "    for t, q in queries.items():\n",
    "        top_words = mc.wordCount(word_query.format(p,q,\" \".join(m)), solr_filter=timespan)\n",
    "        item = {\n",
    "            'place': p,\n",
    "            'topic': t,\n",
    "            'top_words': [{'word': w['term'], 'freq': w['count']} for w in top_words if w['term'] not in [p.lower(), 'climate']]\n",
    "        }\n",
    "        word_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-words.json', 'w') as f:\n",
    "    f.write(json.dumps(word_data))\n",
    "with open('batik-data/batik-words.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow([\"{} - {}\".format(i['place'], i['topic']) for i in word_data])\n",
    "    for idx in range(0, 40):\n",
    "        spamwriter.writerow([i['top_words'][idx]['word'] for i in word_data if idx < len(i['top_words'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Source Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for p, m in places.items():\n",
    "    place_media_count = 0\n",
    "    media_list = []\n",
    "    for tags_id in m:\n",
    "        last_id = 0\n",
    "        media_count = 0\n",
    "        more_media = True\n",
    "        top_media = []\n",
    "        while more_media:\n",
    "            media_page = mc.mediaList(tags_id=tags_id, rows=100, last_media_id=last_id)\n",
    "            media_list += media_page\n",
    "            if len(media_page) == 0:\n",
    "                more_media = False\n",
    "            else:\n",
    "                media_count += len(media_page)\n",
    "                last_id = media_page[-1]['media_id']\n",
    "        place_media_count += media_count\n",
    "    item = {\n",
    "        'place': p,\n",
    "        'media_count': place_media_count,\n",
    "        'top_media': sorted(media_list, key=operator.itemgetter('num_stories_90'), reverse=True)[:20]\n",
    "    }\n",
    "    counts.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-media-counts.json', 'w') as f:\n",
    "    f.write(json.dumps(counts))\n",
    "with open('batik-data/batik-media-counts.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.DictWriter(csvfile, fieldnames=[\"place\", \"media_count\"], extrasaction='ignore')\n",
    "    spamwriter.writeheader()\n",
    "    for row in counts:\n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data = []\n",
    "for p, m in places.items():\n",
    "    coll_list = [mc.tag(tags_id) for tags_id in m]\n",
    "    item = {\n",
    "        'place': p,\n",
    "        'collections': coll_list,\n",
    "    }\n",
    "    collection_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "with open('batik-data/batik-collections.json', 'w') as f:\n",
    "    f.write(json.dumps(collection_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
